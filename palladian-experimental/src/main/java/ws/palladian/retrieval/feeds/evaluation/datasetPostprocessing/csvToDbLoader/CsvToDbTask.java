package ws.palladian.retrieval.feeds.evaluation.datasetPostprocessing.csvToDbLoader;

import java.io.BufferedReader;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.UnsupportedEncodingException;
import java.sql.Timestamp;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Date;
import java.util.HashSet;
import java.util.List;
import java.util.Set;
import java.util.concurrent.Callable;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import ws.palladian.helper.StopWatch;
import ws.palladian.helper.date.DateHelper;
import ws.palladian.helper.io.FileHelper;
import ws.palladian.retrieval.feeds.Feed;
import ws.palladian.retrieval.feeds.FeedTaskResult;
import ws.palladian.retrieval.feeds.evaluation.DatasetCreator;
import ws.palladian.retrieval.feeds.evaluation.EvaluationFeedDatabase;
import ws.palladian.retrieval.feeds.evaluation.disssandro_temp.EvaluationFeedItem;

/**
 * TUDCS6 specific.<br />
 * Load the reconstructed feed entry streams from csv files (created when creating a dataset such as TUDCS6) to
 * database.
 * 
 * Similar to {@link FeedTask}, this class uses a thread to process a single feed. While {@link FeedTask} retrieves the
 * feed from the web, this task iterates over persisted files generated by the {@link DatasetCreator}.
 * 
 * @author Sandro Reichert
 * 
 */
public class CsvToDbTask implements Callable<FeedTaskResult> {

    /** The logger for this class. */
    private final static Logger LOGGER = LoggerFactory.getLogger(CsvToDbTask.class);

    /**
     * The feed to process by this task.
     */
    private Feed feed = null;

    /**
     * The feed DB.
     */
    private final EvaluationFeedDatabase feedDatabase;

    /**
     * Warn if processing of a feed takes longer than this.
     */
    public static final long EXECUTION_WARN_TIME = 3 * DateHelper.MINUTE_MS;

    /**
     * 1999-03-01, It is very unlikely that the an item is older than the RSS 0.9 spec introduced by Netscape
     * (http://backend.userland.com/rss091)
     */
    private static final Date MIN_ITEM_PUBDATE = new Date(920246400000L);

    /**
     * The number of items processed.
     */
    private int itemCounter = 0;

    /**
     * The sequence number written to each item.
     */
    private int sequenceNumber = 0;

    /**
     * Creates a new gz processing task for a provided feed.
     * 
     * @param feed The feed retrieved by this task.
     */
    public CsvToDbTask(Feed dbFeed, EvaluationFeedDatabase feedDatabase) {
        this.feed = dbFeed;
        this.feedDatabase = feedDatabase;
    }

    /** A collection of all intermediate results that can happen, e.g. when updating meta information or a data base. */
    private Set<FeedTaskResult> resultSet = new HashSet<FeedTaskResult>();

    @Override
    public FeedTaskResult call() throws Exception {
        StopWatch timer = new StopWatch();
        try {
            LOGGER.debug("Start processing of feed id " + feed.getId() + " (" + feed.getFeedUrl() + ")");

            // skip feeds that have never been checked. We dont have any files, nor a folder for them.
            if (feed.getChecks() == 0) {
                LOGGER.debug("Feed id " + feed.getId() + " has never been checked. Nothing to do.");
                resultSet.add(FeedTaskResult.SUCCESS);
                doFinalLogging(timer);
                return getResult();
            }

            // skip feeds that contain no item
            if (feed.getNumberOfItemsReceived() == 0) {
                LOGGER.debug("Feed id " + feed.getId() + " has no items. Nothing to do.");
                resultSet.add(FeedTaskResult.SUCCESS);
                doFinalLogging(timer);
                return getResult();
            }

            // get the path of the feed's folder and csv file
            String csvFilePath = DatasetCreator.getCSVFilePath(feed.getId(),
                    DatasetCreator.getSafeFeedName(feed.getFeedUrl()));

            // get items from csv file
            // Caution. some csv files are huge, might be better to stream the file instead of loading it into memory.
            List<String> items = readCsv(csvFilePath);

            // split items into parts
            List<String[]> splitItems = splitItems(items);

            int batchSize = 1000;
            int listCapacity = Math.max(items.size(), batchSize);
            List<EvaluationFeedItem> allItems = new ArrayList<EvaluationFeedItem>(listCapacity);

            // we need 2 variables to a) remember the time of the last poll and b) notice that a new poll is written
            Date lastPolltime = null;
            Date pollTimeLastIteration = null;
            for (String[] splitItem : splitItems) {

                // ignore MISS line
                if (splitItem[0].startsWith("MISS")) {
                    continue;
                }

                try {
                    itemCounter++;

                    // ----- read data from csv -----
                    // get timestamps and windowSize from csv
                    String csvPublishDate = splitItem[0];

                    Date publishDate = null;
                    // set only if entry has a publish time. In case the item had no publish date, we wrote
                    // 0000000000000 to the csv file to indicate a not existing publish date
                    if (!csvPublishDate.equals("0000000000000")) {
                        publishDate = new Date(Long.parseLong(csvPublishDate));
                    }

                    // get pollTimestamp from csv
                    Date currentPollTime = new Date(Long.parseLong(splitItem[1]));

                    // get item hash from csv
                    String hash = splitItem[2];

                    // ----- process data -----

                    // check for new poll, write previous poll data to db
                    // first poll
                    if (pollTimeLastIteration == null) {
                        pollTimeLastIteration = currentPollTime;
                    }
                    // all subsequent polls
                    else if (pollTimeLastIteration.before(currentPollTime)) {
                        // we need to load lastPollTime from table feed_polls since we might have polled without finding
                        // a new entry
                        lastPolltime = feedDatabase.getPreviousFeedPoll(feed.getId(),
                                new Timestamp(currentPollTime.getTime())).getPollTimestamp();
                        pollTimeLastIteration = currentPollTime;
                        addItemsToDb(allItems);
                        allItems = new ArrayList<EvaluationFeedItem>(listCapacity);
                    }

                    Date correctedPublishDate = correctPublishDate(publishDate, currentPollTime, lastPolltime);

                    // create new, minimal item and add to feed
                    EvaluationFeedItem item = new EvaluationFeedItem();
                    item.setFeedId(feed.getId());
                    item.setHash(hash);
                    item.setPublished(publishDate);
                    item.setCorrectedPublishedDate(correctedPublishDate);
                    item.setPollTimestamp(currentPollTime);

                    // keep all items locally
                    allItems.add(item);

                    // // do not waste memory: collect only 1000 items, add them to db and continue. TUDCS6 had up to
                    // // 12 million items per feed...
                    // if (allItems.size() == batchSize) {
                    // addItemsToDb(allItems);
                    // allItems = new ArrayList<EvaluationFeedItem>(listCapacity);
                    // }

                } catch (NumberFormatException e) {
                    LOGGER.error("Could not get number from csv: " + e.getLocalizedMessage());
                }
            }


            if (itemCounter != feed.getNumberOfItemsReceived()) {
                LOGGER.error("Feed id " + feed.getId() + ": feed.getNumberOfItemsReceived() = "
                        + feed.getNumberOfItemsReceived() + " but there were " + itemCounter + " items in csv file!");
                resultSet.add(FeedTaskResult.ERROR);
            } else {
                resultSet.add(FeedTaskResult.SUCCESS);
            }

            // store all Items in db
            doFinalStuff(timer, allItems);
            return getResult();

            // This is ugly but required to catch everything. If we skip this, threads may run much longer till they are
            // killed by the thread pool internals. Errors are logged only and not written to database.
        } catch (Throwable th) {
            LOGGER.error("Error processing feedID " + feed.getId() + ": " + th);
            resultSet.add(FeedTaskResult.ERROR);
            doFinalLogging(timer);
            return getResult();
        }

    }

    /**
     * Correct publish date. multiple, consecutive corrections may occur.
     * 1) if no publish date is present, set to current pollTime
     * 2) if publishDate < 1999-03-01, set to 1999-03-01 00:00:00. It is very unlikely that the
     * post is older than the RSS 0.9 spec introduced by Netscape (http://backend.userland.com/rss091)
     * 3) if publishDate < lastPolltime, set to current pollTime.
     * 4) if publishDate > pollTime, set to current pollTime.
     * 
     * @param origPublishDate The item's original publish date.
     * @param currentPollTime The time of the current poll (that contains the item).
     * @param lastPolltime The time of the last poll, i.e. the newest poll in the past of the current poll.
     * @return The corrected publish date, never <code>null</code>.
     */
    private Date correctPublishDate(Date origPublishDate, Date currentPollTime, Date lastPolltime) {
        Date correctedPublishDate = origPublishDate;
        boolean correctionDone = false;
        if (correctedPublishDate == null) {
            correctedPublishDate = currentPollTime;
            correctionDone = true;
        } else {
            if (correctedPublishDate.before(MIN_ITEM_PUBDATE)) {
                correctedPublishDate = MIN_ITEM_PUBDATE;
                correctionDone = true;
            }
            if (lastPolltime != null && correctedPublishDate.before(lastPolltime)) {
                correctedPublishDate = currentPollTime;
                correctionDone = true;
            } else if (correctedPublishDate.after(currentPollTime)) {
                correctedPublishDate = currentPollTime;
                correctionDone = true;
            }
        }

        if (LOGGER.isDebugEnabled() && correctionDone) {
            LOGGER.debug("Feed id " + feed.getId() + " corrected publish date \"" + origPublishDate + "\" to \""
                    + correctedPublishDate + "\"");
        }

        return correctedPublishDate;
    }

    /**
     * Sets the feed task result and processing time of this task, saves the feed to database, does the final logging
     * and frees the feed's memory.
     * 
     * @param timer The {@link StopWatch} to estimate processing time
     */
    private void doFinalStuff(StopWatch timer, List<EvaluationFeedItem> allItems) {
        if (timer.getElapsedTime() > EXECUTION_WARN_TIME) {
            LOGGER.warn("Processing feed id " + feed.getId() + " took very long: "
                    + timer.getElapsedTimeString());
            resultSet.add(FeedTaskResult.EXECUTION_TIME_WARNING);
        }
        addItemsToDb(allItems);
        doFinalLogging(timer);
        // since the feed is kept in memory we need to remove all items and the document stored in the feed
        feed.freeMemory();
    }

    /**
     * Decide the status of this FeedTask. This is done here to have a fixed ranking on the values.
     * 
     * @return The (current) result of the feed task.
     */
    private FeedTaskResult getResult() {
        FeedTaskResult result = null;
        if (resultSet.contains(FeedTaskResult.ERROR)) {
            result = FeedTaskResult.ERROR;
        } else if (resultSet.contains(FeedTaskResult.UNREACHABLE)) {
            result = FeedTaskResult.UNREACHABLE;
        } else if (resultSet.contains(FeedTaskResult.UNPARSABLE)) {
            result = FeedTaskResult.UNPARSABLE;
        } else if (resultSet.contains(FeedTaskResult.EXECUTION_TIME_WARNING)) {
            result = FeedTaskResult.EXECUTION_TIME_WARNING;
        } else if (resultSet.contains(FeedTaskResult.MISS)) {
            result = FeedTaskResult.MISS;
        } else if (resultSet.contains(FeedTaskResult.SUCCESS)) {
            result = FeedTaskResult.SUCCESS;
        } else {
            result = FeedTaskResult.OPEN;
        }

        return result;
    }

    /**
     * Do final logging of result to error or debug log, depending on the FeedTaskResult.
     * 
     * @param timer the {@link StopWatch} started when started processing the feed.
     */
    private void doFinalLogging(StopWatch timer) {
        FeedTaskResult result = getResult();
        String msg = "Finished processing of feed id " + feed.getId() + ". Result: " + result
                + ". Processing took " + timer.getElapsedTimeString();
        if (result == FeedTaskResult.ERROR) {
            LOGGER.error(msg);
        } else if (LOGGER.isDebugEnabled()) {
            LOGGER.debug(msg);
        }
    }

    /**
     * Save the feed back to the database. In case of database errors, add error to {@link #resultSet}.
     * 
     * @param allItems The list of all items to add.
     */
    private void addItemsToDb(List<EvaluationFeedItem> allItems) {
        Collections.sort(allItems);
        for (EvaluationFeedItem item : allItems) {
            sequenceNumber++;
            item.setSequenceNumber(sequenceNumber);
        }

        boolean dbSuccess = feedDatabase.addEvaluationItems(allItems);
        if (!dbSuccess) {
            resultSet.add(FeedTaskResult.ERROR);
        }
    }

    /**
     * Splits each item into its parts, using ";" as separator.
     * 
     * @param items Items to split
     * @return List with items split into their parts.
     */
    public static List<String[]> splitItems(List<String> items) {
        List<String[]> splitItems = new ArrayList<String[]>();
        for (String item : items) {
            splitItems.add(item.split(";"));
        }
        return splitItems;
    }

    /**
     * Read the csv file into a list of strings, using encoding UTF-8
     * 
     * @param csvPath file to read
     * @return list of csv lines
     */
    public static List<String> readCsv(String csvPath) {
        // List<String> items = FileHelper.readFileToArray(csvPath);
        // return items;

        // FileHelper does not set an encoding explicitly, this causes trouble, when our test cases
        // are run via maven. I suppose the problem has to do with a different default encoding,
        // although everything seems to be configured correctly at first glance. We should think whether
        // it makes sense to always explicitly set UTF-8 when reading files. -- Philipp.
        BufferedReader reader = null;
        List<String> result = new ArrayList<String>();
        try {
            reader = new BufferedReader(new InputStreamReader(new FileInputStream(csvPath), "UTF-8"));
            String line = null;
            while ((line = reader.readLine()) != null) {
                result.add(line);
            }
        } catch (FileNotFoundException e) {
            LOGGER.error("", e);
        } catch (UnsupportedEncodingException e) {
            LOGGER.error("", e);
        } catch (IOException e) {
            LOGGER.error("", e);
        } finally {
            FileHelper.close(reader);
        }
        return result;

    }

}
